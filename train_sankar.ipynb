{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import data_loader\n",
    "from models.sankar import *\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nishant/Desktop/Spring 2021/NASA-Intern/Codes/nishant-trials'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 2\n",
    "LR = 2e-3\n",
    "\n",
    "N_CLASSES = 10\n",
    "NUM_FILTERS_DISC = 64\n",
    "NUM_FILTERS_GEN = 64\n",
    "Z_DIM = 512\n",
    "N_DIM = 2 * NUM_FILTERS_DISC\n",
    "\n",
    "ADV_WEIGHT = 0.1\n",
    "ALPHA = 0.3\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "transform_m = transforms.Compose(\n",
    "    [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset_m = datasets.MNIST(\n",
    "    \"./data/mnist\", train=True, download=False, transform=transform_m\n",
    ")\n",
    "trainloader_m = torch.utils.data.DataLoader(\n",
    "    trainset_m, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "testset_m = datasets.MNIST(\n",
    "    \"./data/mnist\", train=False, download=False, transform=transform_m\n",
    ")\n",
    "testloader_m = torch.utils.data.DataLoader(\n",
    "    testset_m, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# MNIST-M\n",
    "transform_mm = transforms.Compose(\n",
    "    [transforms.Resize(32), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "DATA_DIR = \"data/mnist_m/processed/\"\n",
    "\n",
    "trainloader_mm = data_loader.fetch(\n",
    "    data_dir=os.path.join(DATA_DIR, \"mnist_m_train.pt\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    transform=transform_mm,\n",
    ")\n",
    "\n",
    "testloader_mm = data_loader.fetch(\n",
    "    data_dir=os.path.join(DATA_DIR, \"mnist_m_test.pt\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    transform=transform_mm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = min(len(trainloader_m), len(trainloader_mm)) # ~60000/batch_size\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G = Generator(N_DIM, Z_DIM, N_CLASSES, NUM_FILTERS_GEN).to(device)\n",
    "net_D = Discriminator(N_CLASSES, NUM_FILTERS_DISC).to(device)\n",
    "net_F = FeatureExtractor(NUM_FILTERS_DISC).to(device)\n",
    "net_C = ClassifierNet(NUM_FILTERS_DISC, N_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierNet(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_G.apply(utils.weights_init)\n",
    "net_D.apply(utils.weights_init)\n",
    "net_F.apply(utils.weights_init)\n",
    "net_C.apply(utils.weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = optim.Adam(net_D.parameters(), lr=LR)\n",
    "optimizer_G = optim.Adam(net_G.parameters(), lr=LR)\n",
    "optimizer_F = optim.Adam(net_F.parameters(), lr=LR)\n",
    "optimizer_C = optim.Adam(net_C.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classifier = nn.CrossEntropyLoss()\n",
    "criterion_domain = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "real_labels = torch.ones(BATCH_SIZE).to(device)\n",
    "fake_labels = torch.zeros(BATCH_SIZE).to(device)\n",
    "\n",
    "print(real_labels.shape)\n",
    "print(fake_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "batch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-792b115fd497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ptenv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ptenv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch+1}\")\n",
    "\n",
    "    net_D.train()\n",
    "    net_G.train()\n",
    "    net_F.train()\n",
    "    net_C.train()\n",
    "\n",
    "    for i, (data_source, data_target) in enumerate(zip(trainloader_m, trainloader_mm)):\n",
    "        print(f\"batch: {i+1}\")\n",
    "\n",
    "        src_inputs, src_labels = data_source\n",
    "        tgt_inputs, _ = data_target\n",
    "\n",
    "        src_inputs = src_inputs.expand(src_inputs.shape[0], 3, 32, 32)\n",
    "\n",
    "        src_inputs, src_labels = src_inputs.to(device), src_labels.to(device)\n",
    "        tgt_inputs = tgt_inputs.to(device)\n",
    "\n",
    "        # Creating one-hot vectors\n",
    "        src_labels_onehot = torch.zeros(BATCH_SIZE, N_CLASSES + 1).to(device)\n",
    "        tgt_labels_onehot = torch.zeros(BATCH_SIZE, N_CLASSES + 1).to(device)\n",
    "\n",
    "        for num in range(BATCH_SIZE):\n",
    "            src_labels_onehot[num, int(src_labels[num])] = 1\n",
    "            tgt_labels_onehot[num, N_CLASSES] = 1\n",
    "\n",
    "        # Updating Discriminator net_D\n",
    "        net_D.zero_grad()\n",
    "        src_embeddings = net_F(src_inputs)  # [-1, 128]\n",
    "        src_noise = noise = torch.randn(BATCH_SIZE, Z_DIM)\n",
    "        src_concat = torch.cat((src_embeddings, src_labels_onehot, src_noise), 1)\n",
    "        src_gen = net_G(src_concat.detach())\n",
    "\n",
    "        tgt_embeddings = net_F(tgt_inputs)\n",
    "        tgt_noise = noise = torch.randn(BATCH_SIZE, Z_DIM)\n",
    "        tgt_concat = torch.cat((tgt_embeddings, src_labels_onehot, src_noise), 1)\n",
    "        tgt_gen = net_G(tgt_concat.detach())\n",
    "\n",
    "        src_D_domain_real, src_D_classes_real = net_D(src_inputs)\n",
    "\n",
    "        loss_src_D_domain_real = criterion_domain(src_D_domain_real, real_labels)\n",
    "        loss_src_D_classes_real = criterion_classifier(src_D_classes_real, src_labels)\n",
    "\n",
    "        src_D_domain_gen_fake, src_D_classes_gen_fake = net_D(src_gen.detach())\n",
    "        loss_src_D_domain_gen_fake = criterion_domain(\n",
    "            src_D_domain_gen_fake, fake_labels\n",
    "        )\n",
    "\n",
    "        tgt_D_domain_gen_fake, _ = net_D(tgt_gen.detach())\n",
    "        loss_tgt_D_domain_gen_fake = criterion_domain(\n",
    "            tgt_D_domain_gen_fake, fake_labels\n",
    "        )\n",
    "\n",
    "        loss_D = (\n",
    "            loss_src_D_domain_real\n",
    "            + loss_src_D_classes_real\n",
    "            + loss_src_D_domain_gen_fake\n",
    "            + loss_tgt_D_domain_gen_fake\n",
    "        )\n",
    "\n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Recompute net_D after gradient update\n",
    "        src_D_domain_gen_fake, src_D_classes_gen_fake = net_D(src_gen)\n",
    "        tgt_D_domain_gen_fake, _ = net_D(tgt_gen)\n",
    "\n",
    "        # Updating Generator net_G\n",
    "        net_G.zero_grad()\n",
    "\n",
    "        loss_G_domain = criterion_domain(src_D_domain_gen_fake, real_labels)\n",
    "        loss_G_classes = criterion_classifier(src_D_classes_gen_fake, src_labels)\n",
    "        loss_G = loss_G_domain + loss_G_classes\n",
    "\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        #optimizer_G.step()\n",
    "\n",
    "        # Update Classifier net C\n",
    "        net_C.zero_grad()\n",
    "\n",
    "        output_C = net_C(src_embeddings)\n",
    "        loss_C = criterion_classifier(output_C, src_labels)\n",
    "\n",
    "        loss_C.backward(retain_graph=True)\n",
    "        #optimizer_C.step()\n",
    "\n",
    "        # Update Feature Extractor F\n",
    "        net_F.zero_grad()\n",
    "\n",
    "        loss_F_src = ADV_WEIGHT * criterion_classifier(\n",
    "            src_D_classes_gen_fake, src_labels\n",
    "        )\n",
    "        loss_F_tgt = (\n",
    "            ADV_WEIGHT * ALPHA * criterion_domain(tgt_D_domain_gen_fake, real_labels)\n",
    "        )\n",
    "\n",
    "        loss_F = loss_C + loss_F_src + loss_F_tgt\n",
    "        \n",
    "#         print(loss_F)\n",
    "\n",
    "        loss_F.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "        optimizer_C.step()\n",
    "        optimizer_F.step()\n",
    "        \n",
    "        running_loss_total += loss_C\n",
    "\n",
    "        if i % 300 == 0:\n",
    "          print(f\"Epoch: {epoch}/{EPOCHS} Batch: {i}/{num_batches}\")\n",
    "          print(f\"Classifier Training Loss: {running_loss_total/batch}\")\n",
    "          \n",
    "\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0 \n",
    "    accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "            net.eval()\n",
    "            for imgs, lbls in testloader_mm:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                #print(logits.shape,lbls.shape)\n",
    "                logits, _ = net_C(imgs, lamda)\n",
    "                #print(logits.shape)\n",
    "                #print(lbls.shape)\n",
    "                #lbls = lbls.view(*logits.shape)\n",
    "                #print(logits.shape,lbls.shape)\n",
    "                test_loss += criterion_l(logits, lbls)\n",
    "\n",
    "                #logits to probabilities using softmax\n",
    "                ps = torch.exp(logits) / (torch.sum(torch.exp(logits)))\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                #print(top_p.shape, top_class.shape)\n",
    "                equals = top_class == lbls.view(*top_class.shape)\n",
    "                #print(top_class,lbls.view(*top_class.shape))\n",
    "                accuracy += torch.mean(equals.float())\n",
    "\n",
    "    test_accuracy.append(accuracy / len(testloader_mm))\n",
    "    print(f\"Test accuracy: {accuracy / len(testloader_mm)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import *\n",
    "\n",
    "# make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'data'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from scipy.io import loadmat\n",
    "# x = loadmat('data/USPS_all.mat')\n",
    "# x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1100, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = x['data'][:,141,1].reshape(16,16)\n",
    "# y = y.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8f94560a0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
