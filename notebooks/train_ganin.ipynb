{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1616048324174,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "NoaiIAvQCndk",
    "outputId": "faec71cd-8009-4e08-a818-73c154adfb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1616048813278,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "lCbtyJtYhka_",
    "outputId": "0e75ebb1-a8db-4518-ceca-df7b5efa2273"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NASA_Transfer_Learning\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import data_loader\n",
    "from models.ganin import GaninModel\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2 ** 32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2 ** 32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2 ** 32 - 1)\n",
    "\n",
    "# Pytorch performance tuninng guide - NVIDIA\n",
    "torch.backends.cudnn.benchmark = True  # speeds up convolution operations\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1616048324863,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "KhyykiwQgcjc"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5049,
     "status": "ok",
     "timestamp": 1616048328481,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "VRZRaQuHgWvW",
    "outputId": "99668a6d-039d-4227-e0a5-1b635becf71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "No. of Batches:  1875\n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "transform_m = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset_m = datasets.MNIST(\n",
    "    \"data/mnist\", train=True, download=False, transform=transform_m\n",
    ")\n",
    "trainloader_m = torch.utils.data.DataLoader(\n",
    "    trainset_m, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "testset_m = datasets.MNIST(\n",
    "    \"data/mnist\", train=False, download=False, transform=transform_m\n",
    ")\n",
    "testloader_m = torch.utils.data.DataLoader(\n",
    "    testset_m, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "# MNIST-M\n",
    "transform_mm = transforms.Compose(\n",
    "    [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "DATA_DIR = \"data/mnist_m/processed/\"\n",
    "\n",
    "trainloader_mm = data_loader.fetch(\n",
    "    data_dir=os.path.join(DATA_DIR, \"mnist_m_train.pt\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    transform=transform_mm,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "testloader_mm = data_loader.fetch(\n",
    "    data_dir=os.path.join(DATA_DIR, \"mnist_m_test.pt\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    transform=transform_mm,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "net = GaninModel().to(device)\n",
    "\n",
    "criterion_l = nn.CrossEntropyLoss()\n",
    "criterion_d = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "num_batches = min(len(trainloader_m), len(trainloader_mm))  # ~60000/batch_size\n",
    "print(\"No. of Batches: \", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5044,
     "status": "ok",
     "timestamp": 1616048328482,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "e5ho9dRaisl6",
    "outputId": "c7991653-93d8-4f63-8b30-059e98847c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-16GB\n",
      "Memory Allocated: (GB)\n",
      "Allocated:  0.0\n",
      "Cached:  0.0\n"
     ]
    }
   ],
   "source": [
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Allocated: (GB)\")\n",
    "    print(\"Allocated: \", round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1))\n",
    "    print(\"Cached: \", round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489036,
     "status": "ok",
     "timestamp": 1616048812480,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "FirNI_6hjNHw",
    "outputId": "220d621a-0459-4d6f-eefd-263507ce3568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.0\n",
      "Epoch: 0/15 Batch: 300/1875\n",
      "Total Loss: 2.290130376815796\n",
      "Label Loss: 0.917344868183136\n",
      "Domain Loss: 1.3727856874465942\n",
      "Epoch: 0/15 Batch: 600/1875\n",
      "Total Loss: 1.9377704858779907\n",
      "Label Loss: 0.5933164358139038\n",
      "Domain Loss: 1.3444538116455078\n",
      "Epoch: 0/15 Batch: 900/1875\n",
      "Total Loss: 1.7587906122207642\n",
      "Label Loss: 0.45895248651504517\n",
      "Domain Loss: 1.2998383045196533\n",
      "Epoch: 0/15 Batch: 1200/1875\n",
      "Total Loss: 1.6209685802459717\n",
      "Label Loss: 0.38094764947891235\n",
      "Domain Loss: 1.2400199174880981\n",
      "Epoch: 0/15 Batch: 1500/1875\n",
      "Total Loss: 1.50449538230896\n",
      "Label Loss: 0.33010706305503845\n",
      "Domain Loss: 1.174387812614441\n",
      "Epoch: 0/15 Batch: 1800/1875\n",
      "Total Loss: 1.4023813009262085\n",
      "Label Loss: 0.2956162989139557\n",
      "Domain Loss: 1.1067640781402588\n",
      "Test accuracy: 0.47783544659614563\n",
      "\n",
      "\n",
      "alpha: 0.32151273753163445\n",
      "Epoch: 1/15 Batch: 300/1875\n",
      "Total Loss: 1.341217279434204\n",
      "Label Loss: 0.13582591712474823\n",
      "Domain Loss: 1.2053916454315186\n",
      "Epoch: 1/15 Batch: 600/1875\n",
      "Total Loss: 1.3551908731460571\n",
      "Label Loss: 0.13277779519557953\n",
      "Domain Loss: 1.2224130630493164\n",
      "Epoch: 1/15 Batch: 900/1875\n",
      "Total Loss: 1.3481076955795288\n",
      "Label Loss: 0.12830081582069397\n",
      "Domain Loss: 1.2198063135147095\n",
      "Epoch: 1/15 Batch: 1200/1875\n",
      "Total Loss: 1.3430759906768799\n",
      "Label Loss: 0.1273009032011032\n",
      "Domain Loss: 1.2157732248306274\n",
      "Epoch: 1/15 Batch: 1500/1875\n",
      "Total Loss: 1.335483431816101\n",
      "Label Loss: 0.12459555268287659\n",
      "Domain Loss: 1.2108862400054932\n",
      "Epoch: 1/15 Batch: 1800/1875\n",
      "Total Loss: 1.323436975479126\n",
      "Label Loss: 0.12233024835586548\n",
      "Domain Loss: 1.201104760169983\n",
      "Test accuracy: 0.6301916837692261\n",
      "\n",
      "\n",
      "alpha: 0.5827829453479101\n",
      "Epoch: 2/15 Batch: 300/1875\n",
      "Total Loss: 1.3783040046691895\n",
      "Label Loss: 0.12491244077682495\n",
      "Domain Loss: 1.2533918619155884\n",
      "Epoch: 2/15 Batch: 600/1875\n",
      "Total Loss: 1.4287331104278564\n",
      "Label Loss: 0.13458657264709473\n",
      "Domain Loss: 1.2941465377807617\n",
      "Epoch: 2/15 Batch: 900/1875\n",
      "Total Loss: 1.4459316730499268\n",
      "Label Loss: 0.14065484702587128\n",
      "Domain Loss: 1.3052763938903809\n",
      "Epoch: 2/15 Batch: 1200/1875\n",
      "Total Loss: 1.4383207559585571\n",
      "Label Loss: 0.13796959817409515\n",
      "Domain Loss: 1.3003511428833008\n",
      "Epoch: 2/15 Batch: 1500/1875\n",
      "Total Loss: 1.4197649955749512\n",
      "Label Loss: 0.1341903954744339\n",
      "Domain Loss: 1.2855758666992188\n",
      "Epoch: 2/15 Batch: 1800/1875\n",
      "Total Loss: 1.4162776470184326\n",
      "Label Loss: 0.13247047364711761\n",
      "Domain Loss: 1.2838096618652344\n",
      "Test accuracy: 0.7343250513076782\n",
      "\n",
      "\n",
      "alpha: 0.7615941559557646\n",
      "Epoch: 3/15 Batch: 300/1875\n",
      "Total Loss: 1.4362866878509521\n",
      "Label Loss: 0.13580955564975739\n",
      "Domain Loss: 1.3004767894744873\n",
      "Epoch: 3/15 Batch: 600/1875\n",
      "Total Loss: 1.4334101676940918\n",
      "Label Loss: 0.13770371675491333\n",
      "Domain Loss: 1.2957062721252441\n",
      "Epoch: 3/15 Batch: 900/1875\n",
      "Total Loss: 1.4341435432434082\n",
      "Label Loss: 0.13766402006149292\n",
      "Domain Loss: 1.2964797019958496\n",
      "Epoch: 3/15 Batch: 1200/1875\n",
      "Total Loss: 1.436957836151123\n",
      "Label Loss: 0.13589467108249664\n",
      "Domain Loss: 1.301063060760498\n",
      "Epoch: 3/15 Batch: 1500/1875\n",
      "Total Loss: 1.4322758913040161\n",
      "Label Loss: 0.1327666938304901\n",
      "Domain Loss: 1.29951012134552\n",
      "Epoch: 3/15 Batch: 1800/1875\n",
      "Total Loss: 1.428133487701416\n",
      "Label Loss: 0.13095352053642273\n",
      "Domain Loss: 1.297181487083435\n",
      "Test accuracy: 0.8350638747215271\n",
      "\n",
      "\n",
      "alpha: 0.870061661742672\n",
      "Epoch: 4/15 Batch: 300/1875\n",
      "Total Loss: 1.4242565631866455\n",
      "Label Loss: 0.12238282710313797\n",
      "Domain Loss: 1.3018743991851807\n",
      "Epoch: 4/15 Batch: 600/1875\n",
      "Total Loss: 1.4327600002288818\n",
      "Label Loss: 0.12298014760017395\n",
      "Domain Loss: 1.309780240058899\n",
      "Epoch: 4/15 Batch: 900/1875\n",
      "Total Loss: 1.43337082862854\n",
      "Label Loss: 0.12159369885921478\n",
      "Domain Loss: 1.3117775917053223\n",
      "Epoch: 4/15 Batch: 1200/1875\n",
      "Total Loss: 1.432814121246338\n",
      "Label Loss: 0.12244317680597305\n",
      "Domain Loss: 1.3103725910186768\n",
      "Epoch: 4/15 Batch: 1500/1875\n",
      "Total Loss: 1.4360753297805786\n",
      "Label Loss: 0.1233997792005539\n",
      "Domain Loss: 1.3126771450042725\n",
      "Epoch: 4/15 Batch: 1800/1875\n",
      "Total Loss: 1.4366084337234497\n",
      "Label Loss: 0.12275347858667374\n",
      "Domain Loss: 1.3138585090637207\n",
      "Test accuracy: 0.860223650932312\n",
      "\n",
      "\n",
      "alpha: 0.9311096086675774\n",
      "Epoch: 5/15 Batch: 300/1875\n",
      "Total Loss: 1.4616706371307373\n",
      "Label Loss: 0.13273654878139496\n",
      "Domain Loss: 1.3289350271224976\n",
      "Epoch: 5/15 Batch: 600/1875\n",
      "Total Loss: 1.4491409063339233\n",
      "Label Loss: 0.12459166347980499\n",
      "Domain Loss: 1.3245497941970825\n",
      "Epoch: 5/15 Batch: 900/1875\n",
      "Total Loss: 1.432763695716858\n",
      "Label Loss: 0.1214686781167984\n",
      "Domain Loss: 1.3112950325012207\n",
      "Epoch: 5/15 Batch: 1200/1875\n",
      "Total Loss: 1.4206042289733887\n",
      "Label Loss: 0.11580074578523636\n",
      "Domain Loss: 1.3048042058944702\n",
      "Epoch: 5/15 Batch: 1500/1875\n",
      "Total Loss: 1.419714331626892\n",
      "Label Loss: 0.11334536969661713\n",
      "Domain Loss: 1.3063700199127197\n",
      "Epoch: 5/15 Batch: 1800/1875\n",
      "Total Loss: 1.420681357383728\n",
      "Label Loss: 0.1137363463640213\n",
      "Domain Loss: 1.3069452047348022\n",
      "Test accuracy: 0.8638178706169128\n",
      "\n",
      "\n",
      "alpha: 0.9640275800758169\n",
      "Epoch: 6/15 Batch: 300/1875\n",
      "Total Loss: 1.425322413444519\n",
      "Label Loss: 0.11203164607286453\n",
      "Domain Loss: 1.3132903575897217\n",
      "Epoch: 6/15 Batch: 600/1875\n",
      "Total Loss: 1.424358606338501\n",
      "Label Loss: 0.11038116365671158\n",
      "Domain Loss: 1.3139774799346924\n",
      "Epoch: 6/15 Batch: 900/1875\n",
      "Total Loss: 1.4229923486709595\n",
      "Label Loss: 0.1085486188530922\n",
      "Domain Loss: 1.3144450187683105\n",
      "Epoch: 6/15 Batch: 1200/1875\n",
      "Total Loss: 1.4227685928344727\n",
      "Label Loss: 0.1077718734741211\n",
      "Domain Loss: 1.3149967193603516\n",
      "Epoch: 6/15 Batch: 1500/1875\n",
      "Total Loss: 1.4228826761245728\n",
      "Label Loss: 0.1092129796743393\n",
      "Domain Loss: 1.313670039176941\n",
      "Epoch: 6/15 Batch: 1800/1875\n",
      "Total Loss: 1.426303505897522\n",
      "Label Loss: 0.11192400008440018\n",
      "Domain Loss: 1.314379334449768\n",
      "Test accuracy: 0.8740015625953674\n",
      "\n",
      "\n",
      "alpha: 0.9813680813098666\n",
      "Epoch: 7/15 Batch: 300/1875\n",
      "Total Loss: 1.4078123569488525\n",
      "Label Loss: 0.09735996276140213\n",
      "Domain Loss: 1.3104517459869385\n",
      "Epoch: 7/15 Batch: 600/1875\n",
      "Total Loss: 1.414515495300293\n",
      "Label Loss: 0.10211539268493652\n",
      "Domain Loss: 1.3123984336853027\n",
      "Epoch: 7/15 Batch: 900/1875\n",
      "Total Loss: 1.4209940433502197\n",
      "Label Loss: 0.10599465668201447\n",
      "Domain Loss: 1.314998984336853\n",
      "Epoch: 7/15 Batch: 1200/1875\n",
      "Total Loss: 1.430003046989441\n",
      "Label Loss: 0.11234591156244278\n",
      "Domain Loss: 1.3176573514938354\n",
      "Epoch: 7/15 Batch: 1500/1875\n",
      "Total Loss: 1.424614429473877\n",
      "Label Loss: 0.11053333431482315\n",
      "Domain Loss: 1.314081072807312\n",
      "Epoch: 7/15 Batch: 1800/1875\n",
      "Total Loss: 1.4219703674316406\n",
      "Label Loss: 0.10915663838386536\n",
      "Domain Loss: 1.3128129243850708\n",
      "Test accuracy: 0.8600239753723145\n",
      "\n",
      "\n",
      "alpha: 0.9903904942256809\n",
      "Epoch: 8/15 Batch: 300/1875\n",
      "Total Loss: 1.4187581539154053\n",
      "Label Loss: 0.10418228805065155\n",
      "Domain Loss: 1.3145755529403687\n",
      "Epoch: 8/15 Batch: 600/1875\n",
      "Total Loss: 1.4153746366500854\n",
      "Label Loss: 0.1041695699095726\n",
      "Domain Loss: 1.311204195022583\n",
      "Epoch: 8/15 Batch: 900/1875\n",
      "Total Loss: 1.420531988143921\n",
      "Label Loss: 0.10684911161661148\n",
      "Domain Loss: 1.313682198524475\n",
      "Epoch: 8/15 Batch: 1200/1875\n",
      "Total Loss: 1.4236462116241455\n",
      "Label Loss: 0.10875337570905685\n",
      "Domain Loss: 1.3148940801620483\n",
      "Epoch: 8/15 Batch: 1500/1875\n",
      "Total Loss: 1.423635721206665\n",
      "Label Loss: 0.10831703245639801\n",
      "Domain Loss: 1.3153184652328491\n",
      "Epoch: 8/15 Batch: 1800/1875\n",
      "Total Loss: 1.423907995223999\n",
      "Label Loss: 0.10688627511262894\n",
      "Domain Loss: 1.3170208930969238\n",
      "Test accuracy: 0.856829047203064\n",
      "\n",
      "\n",
      "alpha: 0.9950547536867307\n",
      "Epoch: 9/15 Batch: 300/1875\n",
      "Total Loss: 1.4106354713439941\n",
      "Label Loss: 0.09637326002120972\n",
      "Domain Loss: 1.314261794090271\n",
      "Epoch: 9/15 Batch: 600/1875\n",
      "Total Loss: 1.4156935214996338\n",
      "Label Loss: 0.10183528810739517\n",
      "Domain Loss: 1.3138571977615356\n",
      "Epoch: 9/15 Batch: 900/1875\n",
      "Total Loss: 1.4160906076431274\n",
      "Label Loss: 0.10220345109701157\n",
      "Domain Loss: 1.3138861656188965\n",
      "Epoch: 9/15 Batch: 1200/1875\n",
      "Total Loss: 1.4164824485778809\n",
      "Label Loss: 0.10127642005681992\n",
      "Domain Loss: 1.3152053356170654\n",
      "Epoch: 9/15 Batch: 1500/1875\n",
      "Total Loss: 1.4179269075393677\n",
      "Label Loss: 0.10200860351324081\n",
      "Domain Loss: 1.3159180879592896\n",
      "Epoch: 9/15 Batch: 1800/1875\n",
      "Total Loss: 1.4191813468933105\n",
      "Label Loss: 0.10274633765220642\n",
      "Domain Loss: 1.3164349794387817\n",
      "Test accuracy: 0.8722044825553894\n",
      "\n",
      "\n",
      "alpha: 0.9974579674738373\n",
      "Epoch: 10/15 Batch: 300/1875\n",
      "Total Loss: 1.4243245124816895\n",
      "Label Loss: 0.09832103550434113\n",
      "Domain Loss: 1.3260027170181274\n",
      "Epoch: 10/15 Batch: 600/1875\n",
      "Total Loss: 1.417030930519104\n",
      "Label Loss: 0.09395840018987656\n",
      "Domain Loss: 1.323072910308838\n",
      "Epoch: 10/15 Batch: 900/1875\n",
      "Total Loss: 1.4120569229125977\n",
      "Label Loss: 0.09331472218036652\n",
      "Domain Loss: 1.3187440633773804\n",
      "Epoch: 10/15 Batch: 1200/1875\n",
      "Total Loss: 1.4124658107757568\n",
      "Label Loss: 0.0947560966014862\n",
      "Domain Loss: 1.3177111148834229\n",
      "Epoch: 10/15 Batch: 1500/1875\n",
      "Total Loss: 1.4134876728057861\n",
      "Label Loss: 0.09663330763578415\n",
      "Domain Loss: 1.3168556690216064\n",
      "Epoch: 10/15 Batch: 1800/1875\n",
      "Total Loss: 1.4179774522781372\n",
      "Label Loss: 0.09770363569259644\n",
      "Domain Loss: 1.3202728033065796\n",
      "Test accuracy: 0.8918730020523071\n",
      "\n",
      "\n",
      "alpha: 0.9986940693248945\n",
      "Epoch: 11/15 Batch: 300/1875\n",
      "Total Loss: 1.4122439622879028\n",
      "Label Loss: 0.09653577208518982\n",
      "Domain Loss: 1.3157075643539429\n",
      "Epoch: 11/15 Batch: 600/1875\n",
      "Total Loss: 1.41316556930542\n",
      "Label Loss: 0.0956803411245346\n",
      "Domain Loss: 1.317483901977539\n",
      "Epoch: 11/15 Batch: 900/1875\n",
      "Total Loss: 1.414771318435669\n",
      "Label Loss: 0.09869302809238434\n",
      "Domain Loss: 1.316078782081604\n",
      "Epoch: 11/15 Batch: 1200/1875\n",
      "Total Loss: 1.4127357006072998\n",
      "Label Loss: 0.09669353067874908\n",
      "Domain Loss: 1.3160423040390015\n",
      "Epoch: 11/15 Batch: 1500/1875\n",
      "Total Loss: 1.4133883714675903\n",
      "Label Loss: 0.09738610684871674\n",
      "Domain Loss: 1.3160006999969482\n",
      "Epoch: 11/15 Batch: 1800/1875\n",
      "Total Loss: 1.4147570133209229\n",
      "Label Loss: 0.09798695892095566\n",
      "Domain Loss: 1.316767692565918\n",
      "Test accuracy: 0.8533346652984619\n",
      "\n",
      "\n",
      "alpha: 0.9993292997390673\n",
      "Epoch: 12/15 Batch: 300/1875\n",
      "Total Loss: 1.4246091842651367\n",
      "Label Loss: 0.09839818626642227\n",
      "Domain Loss: 1.3262114524841309\n",
      "Epoch: 12/15 Batch: 600/1875\n",
      "Total Loss: 1.4282225370407104\n",
      "Label Loss: 0.0984192043542862\n",
      "Domain Loss: 1.3298040628433228\n",
      "Epoch: 12/15 Batch: 900/1875\n",
      "Total Loss: 1.4288808107376099\n",
      "Label Loss: 0.09982039779424667\n",
      "Domain Loss: 1.32906174659729\n",
      "Epoch: 12/15 Batch: 1200/1875\n",
      "Total Loss: 1.4269146919250488\n",
      "Label Loss: 0.09861067682504654\n",
      "Domain Loss: 1.3283053636550903\n",
      "Epoch: 12/15 Batch: 1500/1875\n",
      "Total Loss: 1.4264482259750366\n",
      "Label Loss: 0.09869123250246048\n",
      "Domain Loss: 1.3277578353881836\n",
      "Epoch: 12/15 Batch: 1800/1875\n",
      "Total Loss: 1.4254741668701172\n",
      "Label Loss: 0.09811674803495407\n",
      "Domain Loss: 1.3273590803146362\n",
      "Test accuracy: 0.865015983581543\n",
      "\n",
      "\n",
      "alpha: 0.9996555948057622\n",
      "Epoch: 13/15 Batch: 300/1875\n",
      "Total Loss: 1.423453688621521\n",
      "Label Loss: 0.10296039283275604\n",
      "Domain Loss: 1.3204926252365112\n",
      "Epoch: 13/15 Batch: 600/1875\n",
      "Total Loss: 1.405997395515442\n",
      "Label Loss: 0.090514175593853\n",
      "Domain Loss: 1.3154826164245605\n",
      "Epoch: 13/15 Batch: 900/1875\n",
      "Total Loss: 1.405835509300232\n",
      "Label Loss: 0.09155548363924026\n",
      "Domain Loss: 1.3142796754837036\n",
      "Epoch: 13/15 Batch: 1200/1875\n",
      "Total Loss: 1.4070887565612793\n",
      "Label Loss: 0.09084344655275345\n",
      "Domain Loss: 1.3162449598312378\n",
      "Epoch: 13/15 Batch: 1500/1875\n",
      "Total Loss: 1.4138853549957275\n",
      "Label Loss: 0.09369557350873947\n",
      "Domain Loss: 1.320189356803894\n",
      "Epoch: 13/15 Batch: 1800/1875\n",
      "Total Loss: 1.4135944843292236\n",
      "Label Loss: 0.09461561590433121\n",
      "Domain Loss: 1.3189802169799805\n",
      "Test accuracy: 0.8398562073707581\n",
      "\n",
      "\n",
      "alpha: 0.9998231616599622\n",
      "Epoch: 14/15 Batch: 300/1875\n",
      "Total Loss: 1.4433118104934692\n",
      "Label Loss: 0.10152864456176758\n",
      "Domain Loss: 1.3417834043502808\n",
      "Epoch: 14/15 Batch: 600/1875\n",
      "Total Loss: 1.429481029510498\n",
      "Label Loss: 0.09620000422000885\n",
      "Domain Loss: 1.333280324935913\n",
      "Epoch: 14/15 Batch: 900/1875\n",
      "Total Loss: 1.4246606826782227\n",
      "Label Loss: 0.0973743125796318\n",
      "Domain Loss: 1.3272857666015625\n",
      "Epoch: 14/15 Batch: 1200/1875\n",
      "Total Loss: 1.4207850694656372\n",
      "Label Loss: 0.09577928483486176\n",
      "Domain Loss: 1.325005292892456\n",
      "Epoch: 14/15 Batch: 1500/1875\n",
      "Total Loss: 1.4235402345657349\n",
      "Label Loss: 0.09665150940418243\n",
      "Domain Loss: 1.3268879652023315\n",
      "Epoch: 14/15 Batch: 1800/1875\n",
      "Total Loss: 1.4199292659759521\n",
      "Label Loss: 0.09537874907255173\n",
      "Domain Loss: 1.3245508670806885\n",
      "Test accuracy: 0.8674121499061584\n",
      "\n",
      "\n",
      "Training Time for 15 epochs: 0:08:04.001516\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    running_loss_total = 0\n",
    "    running_loss_l = 0\n",
    "    running_loss_d = 0\n",
    "\n",
    "    dataiter_mm = iter(trainloader_mm)\n",
    "    dataiter_m = iter(trainloader_m)\n",
    "    alpha = (2 / (1 + np.exp(-10 * ((epoch + 0.0) / EPOCHS)))) - 1\n",
    "    print(f\"alpha: {alpha}\")\n",
    "\n",
    "    net.train()\n",
    "    for batch in range(1, num_batches + 1):\n",
    "        loss_total = 0\n",
    "        loss_d = 0\n",
    "        loss_l = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # for source domain\n",
    "        imgs, lbls = dataiter_m.next()\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        imgs = torch.cat((imgs, imgs, imgs), 1)\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        out_l, out_d = net(imgs, alpha)\n",
    "        loss_l_src = criterion_l(out_l, lbls)\n",
    "        actual_d = torch.zeros(out_d.shape).to(device)\n",
    "        loss_d_src = criterion_d(out_d, actual_d)\n",
    "\n",
    "        # for target domain\n",
    "        imgs, lbls = dataiter_mm.next()\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        _, out_d = net(imgs, alpha)\n",
    "        actual_d = torch.ones(out_d.shape).to(device)\n",
    "        loss_d_tgt = criterion_d(out_d, actual_d)\n",
    "\n",
    "        loss_total = loss_d_src + loss_l_src + loss_d_tgt\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_total += loss_total\n",
    "        running_loss_d += loss_d_src + loss_d_tgt\n",
    "        running_loss_l += loss_l_src\n",
    "\n",
    "        if batch % 300 == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS} Batch: {batch}/{num_batches}\")\n",
    "            print(f\"Total Loss: {running_loss_total/batch}\")\n",
    "        #   print(f\"Label Loss: {running_loss_l/batch}\")\n",
    "        #   print(f\"Domain Loss: {running_loss_d/batch}\")\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for imgs, lbls in testloader_mm:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            # print(imgs.shape)\n",
    "            # print(lbls.shape)\n",
    "\n",
    "            logits, _ = net(imgs, alpha=0)\n",
    "            # print(logits.shape)\n",
    "            test_loss += criterion_l(logits, lbls)\n",
    "\n",
    "            # derive which class index corresponds to max value\n",
    "            preds_l = torch.max(logits, dim=1)[\n",
    "                1\n",
    "            ]  # [1]: indices(class) corresponding to max values\n",
    "            equals = torch.eq(preds_l, lbls)  # count no. of correct class predictions\n",
    "            accuracy += torch.mean(equals.float())\n",
    "\n",
    "    test_accuracy.append(accuracy / len(testloader_mm))\n",
    "    print(f\"Test accuracy: {accuracy / len(testloader_mm)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training Time for {EPOCHS} epochs: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 489031,
     "status": "ok",
     "timestamp": 1616048812481,
     "user": {
      "displayName": "Nishant Yadav",
      "photoUrl": "",
      "userId": "02529691709873630386"
     },
     "user_tz": 420
    },
    "id": "8jNDPvXe1CBK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_ganin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}